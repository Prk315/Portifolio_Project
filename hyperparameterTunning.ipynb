{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-1",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "This notebook focuses on advanced model tuning using Bagging and Boosting techniques with GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    BaggingClassifier, BaggingRegressor,\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    AdaBoostClassifier, GradientBoostingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load data from trainAndFeatureEngineer.ipynb\n",
    "X_train = np.load('data/X_train_fe.npy')\n",
    "X_test = np.load('data/X_test_fe.npy')\n",
    "y_train = np.load('data/y_train_encoded.npy')\n",
    "y_test = np.load('data/y_test_encoded.npy')\n",
    "\n",
    "# Load label encoder for reference\n",
    "le = joblib.load('data/label_encoder.joblib')\n",
    "\n",
    "print(f'Training set: {X_train.shape}')\n",
    "print(f'Test set: {X_test.shape}')\n",
    "print(f'Classes: {le.classes_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bagging-header",
   "metadata": {},
   "source": [
    "## Bagging Models\n",
    "\n",
    "Bagging (Bootstrap Aggregating) reduces variance by training multiple models on different subsets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bagging-dt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging with Decision Tree base estimator\n",
    "bagging_dt_params = {\n",
    "    'n_estimators': [10, 25, 50],\n",
    "    'max_samples': [0.5, 0.7, 1.0],\n",
    "    'max_features': [0.5, 0.7, 1.0],\n",
    "    'bootstrap': [True],\n",
    "    'bootstrap_features': [False, True]\n",
    "}\n",
    "\n",
    "bagging_dt = GridSearchCV(\n",
    "    BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(random_state=42, max_depth=10),\n",
    "        random_state=42\n",
    "    ),\n",
    "    bagging_dt_params,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "bagging_dt.fit(X_train, y_train)\n",
    "\n",
    "print(f'\\nBest Bagging (DT) Parameters: {bagging_dt.best_params_}')\n",
    "print(f'Best CV F1 Score: {bagging_dt.best_score_:.4f}')\n",
    "\n",
    "bagging_dt_pred = bagging_dt.predict(X_test)\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, bagging_dt_pred):.4f}')\n",
    "print(f'Test F1 Score: {f1_score(y_test, bagging_dt_pred, average=\"weighted\"):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bagging-regressor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging with DecisionTreeRegressor as base (for comparison)\n",
    "bagging_reg_params = {\n",
    "    'n_estimators': [10, 25, 50],\n",
    "    'max_samples': [0.5, 0.7, 1.0],\n",
    "    'max_features': [0.5, 0.7, 1.0]\n",
    "}\n",
    "\n",
    "bagging_reg = GridSearchCV(\n",
    "    BaggingRegressor(\n",
    "        estimator=DecisionTreeRegressor(random_state=42, max_depth=10),\n",
    "        random_state=42\n",
    "    ),\n",
    "    bagging_reg_params,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "\n",
    "print(f'\\nBest Bagging (Regressor) Parameters: {bagging_reg.best_params_}')\n",
    "print(f'Best CV MSE: {-bagging_reg.best_score_:.4f}')\n",
    "\n",
    "# Convert regression predictions to classification\n",
    "bagging_reg_pred = np.clip(np.round(bagging_reg.predict(X_test)), 0, 2).astype(int)\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, bagging_reg_pred):.4f}')\n",
    "print(f'Test F1 Score: {f1_score(y_test, bagging_reg_pred, average=\"weighted\"):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    rf_params,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f'\\nBest Random Forest Parameters: {rf_grid.best_params_}')\n",
    "print(f'Best CV F1 Score: {rf_grid.best_score_:.4f}')\n",
    "\n",
    "rf_pred = rf_grid.predict(X_test)\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, rf_pred):.4f}')\n",
    "print(f'Test F1 Score: {f1_score(y_test, rf_pred, average=\"weighted\"):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf-regressor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor (for comparison)\n",
    "rf_reg_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_reg_grid = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    rf_reg_params,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_reg_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f'\\nBest RF Regressor Parameters: {rf_reg_grid.best_params_}')\n",
    "print(f'Best CV MSE: {-rf_reg_grid.best_score_:.4f}')\n",
    "\n",
    "# Convert regression predictions to classification\n",
    "rf_reg_pred = np.clip(np.round(rf_reg_grid.predict(X_test)), 0, 2).astype(int)\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, rf_reg_pred):.4f}')\n",
    "print(f'Test F1 Score: {f1_score(y_test, rf_reg_pred, average=\"weighted\"):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boosting-header",
   "metadata": {},
   "source": [
    "## Boosting Models\n",
    "\n",
    "Boosting reduces bias by sequentially training models where each model focuses on the errors of the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaboost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classifier\n",
    "ada_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "    'algorithm': ['SAMME']\n",
    "}\n",
    "\n",
    "ada_grid = GridSearchCV(\n",
    "    AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "        random_state=42\n",
    "    ),\n",
    "    ada_params,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "ada_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f'\\nBest AdaBoost Parameters: {ada_grid.best_params_}')\n",
    "print(f'Best CV F1 Score: {ada_grid.best_score_:.4f}')\n",
    "\n",
    "ada_pred = ada_grid.predict(X_test)\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, ada_pred):.4f}')\n",
    "print(f'Test F1 Score: {f1_score(y_test, ada_pred, average=\"weighted\"):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradient-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "gb_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "gb_grid = GridSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    gb_params,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f'\\nBest Gradient Boosting Parameters: {gb_grid.best_params_}')\n",
    "print(f'Best CV F1 Score: {gb_grid.best_score_:.4f}')\n",
    "\n",
    "gb_pred = gb_grid.predict(X_test)\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, gb_pred):.4f}')\n",
    "print(f'Test F1 Score: {f1_score(y_test, gb_pred, average=\"weighted\"):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgboost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost (if available)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    xgb_params = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    xgb_grid = GridSearchCV(\n",
    "        XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss'),\n",
    "        xgb_params,\n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    xgb_grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(f'\\nBest XGBoost Parameters: {xgb_grid.best_params_}')\n",
    "    print(f'Best CV F1 Score: {xgb_grid.best_score_:.4f}')\n",
    "    \n",
    "    xgb_pred = xgb_grid.predict(X_test)\n",
    "    print(f'Test Accuracy: {accuracy_score(y_test, xgb_pred):.4f}')\n",
    "    print(f'Test F1 Score: {f1_score(y_test, xgb_pred, average=\"weighted\"):.4f}')\n",
    "    \n",
    "    xgb_available = True\n",
    "except ImportError:\n",
    "    print('XGBoost not installed. Skipping...')\n",
    "    xgb_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "results = {\n",
    "    'Model': [\n",
    "        'Bagging (DT)',\n",
    "        'Bagging (Regressor)',\n",
    "        'Random Forest',\n",
    "        'RF Regressor',\n",
    "        'AdaBoost',\n",
    "        'Gradient Boosting'\n",
    "    ],\n",
    "    'CV F1': [\n",
    "        bagging_dt.best_score_,\n",
    "        None,\n",
    "        rf_grid.best_score_,\n",
    "        None,\n",
    "        ada_grid.best_score_,\n",
    "        gb_grid.best_score_\n",
    "    ],\n",
    "    'Test Accuracy': [\n",
    "        accuracy_score(y_test, bagging_dt_pred),\n",
    "        accuracy_score(y_test, bagging_reg_pred),\n",
    "        accuracy_score(y_test, rf_pred),\n",
    "        accuracy_score(y_test, rf_reg_pred),\n",
    "        accuracy_score(y_test, ada_pred),\n",
    "        accuracy_score(y_test, gb_pred)\n",
    "    ],\n",
    "    'Test F1': [\n",
    "        f1_score(y_test, bagging_dt_pred, average='weighted'),\n",
    "        f1_score(y_test, bagging_reg_pred, average='weighted'),\n",
    "        f1_score(y_test, rf_pred, average='weighted'),\n",
    "        f1_score(y_test, rf_reg_pred, average='weighted'),\n",
    "        f1_score(y_test, ada_pred, average='weighted'),\n",
    "        f1_score(y_test, gb_pred, average='weighted')\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Add XGBoost if available\n",
    "if xgb_available:\n",
    "    results['Model'].append('XGBoost')\n",
    "    results['CV F1'].append(xgb_grid.best_score_)\n",
    "    results['Test Accuracy'].append(accuracy_score(y_test, xgb_pred))\n",
    "    results['Test F1'].append(f1_score(y_test, xgb_pred, average='weighted'))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print('='*70)\n",
    "print('HYPERPARAMETER TUNING RESULTS')\n",
    "print('='*70)\n",
    "print(results_df.to_string(index=False))\n",
    "print('='*70)\n",
    "\n",
    "# Find best model\n",
    "best_idx = results_df['Test F1'].idxmax()\n",
    "print(f\"\\nBest Model: {results_df.loc[best_idx, 'Model']}\")\n",
    "print(f\"Test F1 Score: {results_df.loc[best_idx, 'Test F1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best-model-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report for best models\n",
    "print('='*70)\n",
    "print('CLASSIFICATION REPORTS')\n",
    "print('='*70)\n",
    "\n",
    "print('\\n--- Random Forest ---')\n",
    "print(classification_report(y_test, rf_pred, target_names=le.classes_))\n",
    "\n",
    "print('\\n--- Gradient Boosting ---')\n",
    "print(classification_report(y_test, gb_pred, target_names=le.classes_))\n",
    "\n",
    "if xgb_available:\n",
    "    print('\\n--- XGBoost ---')\n",
    "    print(classification_report(y_test, xgb_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all tuned models\n",
    "joblib.dump(bagging_dt.best_estimator_, 'data/bagging_dt.joblib')\n",
    "joblib.dump(bagging_reg.best_estimator_, 'data/bagging_reg.joblib')\n",
    "joblib.dump(rf_grid.best_estimator_, 'data/rf_classifier.joblib')\n",
    "joblib.dump(rf_reg_grid.best_estimator_, 'data/rf_regressor.joblib')\n",
    "joblib.dump(ada_grid.best_estimator_, 'data/adaboost.joblib')\n",
    "joblib.dump(gb_grid.best_estimator_, 'data/gradient_boost.joblib')\n",
    "\n",
    "if xgb_available:\n",
    "    joblib.dump(xgb_grid.best_estimator_, 'data/xgboost.joblib')\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('data/hyperparameter_results.csv', index=False)\n",
    "\n",
    "# Save predictions for visualization\n",
    "predictions = {\n",
    "    'y_test': y_test,\n",
    "    'bagging_dt': bagging_dt_pred,\n",
    "    'bagging_reg': bagging_reg_pred,\n",
    "    'rf': rf_pred,\n",
    "    'rf_reg': rf_reg_pred,\n",
    "    'adaboost': ada_pred,\n",
    "    'gradient_boost': gb_pred\n",
    "}\n",
    "\n",
    "if xgb_available:\n",
    "    predictions['xgboost'] = xgb_pred\n",
    "\n",
    "np.savez('data/predictions.npz', **predictions)\n",
    "\n",
    "print('All models and results saved successfully!')\n",
    "print('\\nSaved files:')\n",
    "print('  - data/bagging_dt.joblib, bagging_reg.joblib')\n",
    "print('  - data/rf_classifier.joblib, rf_regressor.joblib')\n",
    "print('  - data/adaboost.joblib, gradient_boost.joblib')\n",
    "if xgb_available:\n",
    "    print('  - data/xgboost.joblib')\n",
    "print('  - data/hyperparameter_results.csv')\n",
    "print('  - data/predictions.npz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
