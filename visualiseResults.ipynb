{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Results Visualization\n",
    "\n",
    "This notebook visualizes and compares the performance of all trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, auc, RocCurveDisplay,\n",
    "    precision_recall_curve, f1_score, accuracy_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Load data\n",
    "X_train = np.load('data/X_train_fe.npy')\n",
    "X_test = np.load('data/X_test_fe.npy')\n",
    "y_train = np.load('data/y_train_encoded.npy')\n",
    "y_test = np.load('data/y_test_encoded.npy')\n",
    "\n",
    "# Load label encoder\n",
    "le = joblib.load('data/label_encoder.joblib')\n",
    "class_names = le.classes_\n",
    "\n",
    "# Load predictions\n",
    "predictions = np.load('data/predictions.npz')\n",
    "\n",
    "# Load results\n",
    "baseline_results = pd.read_csv('data/baseline_results.csv')\n",
    "comparison_results = pd.read_csv('data/comparison_results.csv')\n",
    "hp_results = pd.read_csv('data/hyperparameter_results.csv')\n",
    "\n",
    "print(f'Classes: {class_names}')\n",
    "print(f'Test set size: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-comparison-header",
   "metadata": {},
   "source": [
    "## Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bar-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = pd.concat([\n",
    "    comparison_results[['Model', 'Test Accuracy', 'Test F1']],\n",
    "    hp_results[['Model', 'Test Accuracy', 'Test F1']]\n",
    "], ignore_index=True)\n",
    "\n",
    "# Sort by Test F1\n",
    "all_results = all_results.sort_values('Test F1', ascending=True)\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Accuracy comparison\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(all_results)))\n",
    "axes[0].barh(all_results['Model'], all_results['Test Accuracy'], color=colors)\n",
    "axes[0].set_xlabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Test Accuracy by Model', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlim(0, 1)\n",
    "for i, v in enumerate(all_results['Test Accuracy']):\n",
    "    axes[0].text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=10)\n",
    "\n",
    "# F1 Score comparison\n",
    "axes[1].barh(all_results['Model'], all_results['Test F1'], color=colors)\n",
    "axes[1].set_xlabel('F1 Score (Weighted)', fontsize=12)\n",
    "axes[1].set_title('Test F1 Score by Model', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlim(0, 1)\n",
    "for i, v in enumerate(all_results['Test F1']):\n",
    "    axes[1].text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nAll Models Ranked by F1 Score:')\n",
    "print(all_results.sort_values('Test F1', ascending=False).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion-header",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models for confusion matrices\n",
    "models = {\n",
    "    'Random Forest': predictions['rf'],\n",
    "    'Gradient Boosting': predictions['gradient_boost'],\n",
    "    'AdaBoost': predictions['adaboost'],\n",
    "    'Bagging (DT)': predictions['bagging_dt']\n",
    "}\n",
    "\n",
    "# Try to load XGBoost predictions\n",
    "if 'xgboost' in predictions.files:\n",
    "    models['XGBoost'] = predictions['xgboost']\n",
    "\n",
    "# Create confusion matrix plots\n",
    "n_models = len(models)\n",
    "fig, axes = plt.subplots(2, (n_models + 1) // 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, preds) in enumerate(models.items()):\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    axes[idx].set_title(f'{name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(len(models), len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.suptitle('Confusion Matrices - Top Models', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roc-header",
   "metadata": {},
   "source": [
    "## ROC Curves (Multi-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roc-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models that can provide probability estimates\n",
    "rf_model = joblib.load('data/rf_classifier.joblib')\n",
    "gb_model = joblib.load('data/gradient_boost.joblib')\n",
    "ada_model = joblib.load('data/adaboost.joblib')\n",
    "\n",
    "# Binarize the output for multi-class ROC\n",
    "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "# Get probability predictions\n",
    "model_probs = {\n",
    "    'Random Forest': rf_model.predict_proba(X_test),\n",
    "    'Gradient Boosting': gb_model.predict_proba(X_test),\n",
    "    'AdaBoost': ada_model.predict_proba(X_test)\n",
    "}\n",
    "\n",
    "# Try XGBoost\n",
    "try:\n",
    "    xgb_model = joblib.load('data/xgboost.joblib')\n",
    "    model_probs['XGBoost'] = xgb_model.predict_proba(X_test)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Plot ROC curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for class_idx in range(n_classes):\n",
    "    ax = axes[class_idx]\n",
    "    \n",
    "    for model_name, y_proba in model_probs.items():\n",
    "        fpr, tpr, _ = roc_curve(y_test_bin[:, class_idx], y_proba[:, class_idx])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=11)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=11)\n",
    "    ax.set_title(f'ROC Curve - {class_names[class_idx]}', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=9)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.suptitle('ROC Curves by Class', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-importance-header",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from Random Forest\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Load feature selector to get selected feature indices\n",
    "selector = joblib.load('data/selector.joblib')\n",
    "preprocessor_fe = joblib.load('data/preprocessor_fe.joblib')\n",
    "\n",
    "# Get feature names\n",
    "feature_names = preprocessor_fe.get_feature_names_out()\n",
    "selected_mask = selector.get_support()\n",
    "selected_features = feature_names[selected_mask]\n",
    "\n",
    "# Create importance DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 25 features\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "top_25 = importance_df.head(25)\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(top_25)))\n",
    "bars = ax.barh(range(len(top_25)), top_25['importance'].values, color=colors)\n",
    "ax.set_yticks(range(len(top_25)))\n",
    "ax.set_yticklabels(top_25['feature'].values)\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_title('Top 25 Feature Importances (Random Forest)', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nTop 10 Most Important Features:')\n",
    "print(importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learning-curves-header",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learning-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning curves for best models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Random Forest learning curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    X_train, y_train,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    scoring='f1_weighted'\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "axes[0].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "axes[0].fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='orange')\n",
    "axes[0].plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n",
    "axes[0].plot(train_sizes, test_mean, 'o-', color='orange', label='Cross-validation Score')\n",
    "axes[0].set_xlabel('Training Set Size', fontsize=11)\n",
    "axes[0].set_ylabel('F1 Score', fontsize=11)\n",
    "axes[0].set_title('Learning Curve - Random Forest', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Gradient Boosting learning curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    X_train, y_train,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    scoring='f1_weighted'\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "axes[1].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "axes[1].fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='orange')\n",
    "axes[1].plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n",
    "axes[1].plot(train_sizes, test_mean, 'o-', color='orange', label='Cross-validation Score')\n",
    "axes[1].set_xlabel('Training Set Size', fontsize=11)\n",
    "axes[1].set_ylabel('F1 Score', fontsize=11)\n",
    "axes[1].set_title('Learning Curve - Gradient Boosting', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(loc='lower right')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/learning_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model summary\n",
    "print('='*70)\n",
    "print('FINAL MODEL COMPARISON SUMMARY')\n",
    "print('='*70)\n",
    "\n",
    "# Sort all results by Test F1\n",
    "final_ranking = all_results.sort_values('Test F1', ascending=False).reset_index(drop=True)\n",
    "final_ranking.index = final_ranking.index + 1  # Start ranking from 1\n",
    "final_ranking.index.name = 'Rank'\n",
    "\n",
    "print('\\nModels Ranked by Test F1 Score:')\n",
    "print(final_ranking.to_string())\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "best_model = final_ranking.iloc[0]\n",
    "print(f\"BEST MODEL: {best_model['Model']}\")\n",
    "print(f\"  - Test Accuracy: {best_model['Test Accuracy']:.4f}\")\n",
    "print(f\"  - Test F1 Score: {best_model['Test F1']:.4f}\")\n",
    "print('='*70)\n",
    "\n",
    "# Key insights\n",
    "print('\\nKEY INSIGHTS:')\n",
    "print('  1. Ensemble methods (Bagging/Boosting) generally outperform simple classifiers')\n",
    "print('  2. Feature engineering improved model performance across all classifiers')\n",
    "print('  3. Gradient Boosting and Random Forest show best generalization')\n",
    "print('  4. Class imbalance affects prediction of \"Enrolled\" students')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final ranking\n",
    "final_ranking.to_csv('data/final_model_ranking.csv')\n",
    "\n",
    "print('\\nAll visualizations saved to data/ directory:')\n",
    "print('  - model_comparison.png')\n",
    "print('  - confusion_matrices.png')\n",
    "print('  - roc_curves.png')\n",
    "print('  - feature_importance.png')\n",
    "print('  - learning_curves.png')\n",
    "print('  - final_model_ranking.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
