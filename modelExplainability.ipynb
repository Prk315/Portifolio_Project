{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Explainability with SHAP\n",
    "\n",
    "Understanding **why** our model makes predictions using SHAP (SHapley Additive exPlanations) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "# Load data and models\n",
    "X_train = np.load('data/X_train_fe.npy')\n",
    "X_test = np.load('data/X_test_fe.npy')\n",
    "y_train = np.load('data/y_train_encoded.npy')\n",
    "y_test = np.load('data/y_test_encoded.npy')\n",
    "\n",
    "# Load best model\n",
    "try:\n",
    "    model = joblib.load('data/best_model_final.joblib')\n",
    "    print('Loaded best_model_final.joblib')\n",
    "except:\n",
    "    model = joblib.load('data/rf_classifier.joblib')\n",
    "    print('Loaded rf_classifier.joblib as fallback')\n",
    "\n",
    "# Load preprocessor and feature names\n",
    "preprocessor_fe = joblib.load('data/preprocessor_fe.joblib')\n",
    "selector = joblib.load('data/selector.joblib')\n",
    "le = joblib.load('data/label_encoder.joblib')\n",
    "\n",
    "# Get feature names\n",
    "feature_names_all = preprocessor_fe.get_feature_names_out()\n",
    "selected_mask = selector.get_support()\n",
    "feature_names = feature_names_all[selected_mask]\n",
    "\n",
    "print(f'\\nModel type: {type(model).__name__}')\n",
    "print(f'Number of features: {len(feature_names)}')\n",
    "print(f'Classes: {le.classes_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize SHAP Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TreeExplainer for tree-based models (faster than KernelExplainer)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Calculate SHAP values for test set (use subset for speed)\n",
    "sample_size = min(500, len(X_test))\n",
    "X_test_sample = X_test[:sample_size]\n",
    "y_test_sample = y_test[:sample_size]\n",
    "\n",
    "print(f'Calculating SHAP values for {sample_size} test samples...')\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "print('Done!')\n",
    "\n",
    "# For multi-class, shap_values is a list of arrays (one per class)\n",
    "print(f'\\nSHAP values shape: {len(shap_values)} classes x {shap_values[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Feature Importance (Summary Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot shows feature importance across all predictions\n",
    "shap.summary_plot(shap_values, X_test_sample, feature_names=feature_names, \n",
    "                 class_names=le.classes_, show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/shap_summary_all_classes.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual summary plots for each class\n",
    "for i, class_name in enumerate(le.classes_):\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values[i], X_test_sample, feature_names=feature_names,\n",
    "                     show=False, plot_type='bar')\n",
    "    plt.title(f'Feature Importance for {class_name}', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'data/shap_importance_{class_name.lower()}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Beeswarm Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beeswarm plot for each class (shows feature value distribution and impact)\n",
    "for i, class_name in enumerate(le.classes_):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values[i], X_test_sample, feature_names=feature_names,\n",
    "                     show=False, max_display=15)\n",
    "    plt.title(f'SHAP Values for {class_name} Prediction', fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'data/shap_beeswarm_{class_name.lower()}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Prediction Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explain some specific predictions\n",
    "# Find examples of each class\n",
    "example_indices = []\n",
    "for class_id in range(len(le.classes_)):\n",
    "    # Get indices where true label matches class_id\n",
    "    indices = np.where(y_test_sample == class_id)[0]\n",
    "    if len(indices) > 0:\n",
    "        # Get predictions for these indices\n",
    "        preds = model.predict(X_test_sample[indices])\n",
    "        # Find a correctly predicted example\n",
    "        correct = np.where(preds == class_id)[0]\n",
    "        if len(correct) > 0:\n",
    "            example_indices.append(indices[correct[0]])\n",
    "        else:\n",
    "            # If none correct, just use first one\n",
    "            example_indices.append(indices[0])\n",
    "\n",
    "print(f'Selected {len(example_indices)} example predictions to explain')\n",
    "for idx in example_indices:\n",
    "    true_class = le.classes_[y_test_sample[idx]]\n",
    "    pred_class = le.classes_[model.predict(X_test_sample[idx:idx+1])[0]]\n",
    "    print(f'  Index {idx}: True={true_class}, Predicted={pred_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall plot for individual predictions\n",
    "for idx in example_indices:\n",
    "    true_class = le.classes_[y_test_sample[idx]]\n",
    "    pred_class_id = model.predict(X_test_sample[idx:idx+1])[0]\n",
    "    pred_class = le.classes_[pred_class_id]\n",
    "    \n",
    "    print(f'\\n--- Explaining prediction for sample {idx} ---')\n",
    "    print(f'True label: {true_class}')\n",
    "    print(f'Predicted: {pred_class}')\n",
    "    \n",
    "    # Create waterfall plot for predicted class\n",
    "    shap_explanation = shap.Explanation(\n",
    "        values=shap_values[pred_class_id][idx],\n",
    "        base_values=explainer.expected_value[pred_class_id],\n",
    "        data=X_test_sample[idx],\n",
    "        feature_names=feature_names\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.plots.waterfall(shap_explanation, max_display=15, show=False)\n",
    "    plt.title(f'Why predicted {pred_class}? (True: {true_class})', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'data/shap_waterfall_sample{idx}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Force Plots (Alternative Individual Explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force plot for a Graduate prediction\n",
    "graduate_idx = example_indices[2] if len(example_indices) > 2 else example_indices[0]\n",
    "pred_class_id = model.predict(X_test_sample[graduate_idx:graduate_idx+1])[0]\n",
    "\n",
    "shap.force_plot(\n",
    "    explainer.expected_value[pred_class_id],\n",
    "    shap_values[pred_class_id][graduate_idx],\n",
    "    X_test_sample[graduate_idx],\n",
    "    feature_names=feature_names,\n",
    "    matplotlib=True,\n",
    "    show=False\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/shap_force_plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence Plots (Feature Interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top features by absolute mean SHAP value\n",
    "mean_abs_shap = np.array([np.abs(sv).mean(axis=0) for sv in shap_values]).mean(axis=0)\n",
    "top_feature_indices = np.argsort(mean_abs_shap)[-5:]\n",
    "top_features = feature_names[top_feature_indices]\n",
    "\n",
    "print('Top 5 most important features overall:')\n",
    "for i, feat in enumerate(reversed(top_features)):\n",
    "    print(f'  {i+1}. {feat}')\n",
    "\n",
    "# Create dependence plots for top features\n",
    "for class_id, class_name in enumerate(le.classes_):\n",
    "    print(f'\\nDependence plots for {class_name} class...')\n",
    "    for feat_idx in top_feature_indices[-3:]:  # Top 3\n",
    "        feat_name = feature_names[feat_idx]\n",
    "        try:\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            shap.dependence_plot(\n",
    "                feat_idx, \n",
    "                shap_values[class_id], \n",
    "                X_test_sample,\n",
    "                feature_names=feature_names,\n",
    "                show=False\n",
    "            )\n",
    "            plt.title(f'{feat_name} impact on {class_name} prediction', fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            safe_feat_name = feat_name.replace('/', '_').replace(' ', '_')[:30]\n",
    "            plt.savefig(f'data/shap_dependence_{class_name}_{safe_feat_name}.png', \n",
    "                       dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f'  Skipping {feat_name}: {str(e)[:50]}')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights from SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average absolute SHAP value per feature per class\n",
    "insights_data = []\n",
    "for class_id, class_name in enumerate(le.classes_):\n",
    "    mean_abs_shap_class = np.abs(shap_values[class_id]).mean(axis=0)\n",
    "    top_5_indices = np.argsort(mean_abs_shap_class)[-5:][::-1]\n",
    "    \n",
    "    print(f'\\n{class_name} - Top 5 most influential features:')\n",
    "    for rank, idx in enumerate(top_5_indices, 1):\n",
    "        feat_name = feature_names[idx]\n",
    "        importance = mean_abs_shap_class[idx]\n",
    "        print(f'  {rank}. {feat_name}: {importance:.4f}')\n",
    "        insights_data.append({\n",
    "            'Class': class_name,\n",
    "            'Rank': rank,\n",
    "            'Feature': feat_name,\n",
    "            'Mean |SHAP|': importance\n",
    "        })\n",
    "\n",
    "# Save insights\n",
    "insights_df = pd.DataFrame(insights_data)\n",
    "insights_df.to_csv('data/shap_insights.csv', index=False)\n",
    "print('\\nSaved SHAP insights to data/shap_insights.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we learned from SHAP:**\n",
    "\n",
    "1. **Most important predictors** (global):\n",
    "   - 2nd semester grades and approved units\n",
    "   - 1st semester performance\n",
    "   - Engineered features like approval_rate\n",
    "\n",
    "2. **Class-specific patterns**:\n",
    "   - **Dropout**: Low grades and few approved units strongly predict dropout\n",
    "   - **Graduate**: High grades and consistent approval rates\n",
    "   - **Enrolled**: Mixed signals, harder to predict (class imbalance)\n",
    "\n",
    "3. **Feature interactions**:\n",
    "   - Dependence plots show non-linear relationships\n",
    "   - Some features have threshold effects\n",
    "\n",
    "4. **Model transparency**:\n",
    "   - Waterfall plots explain individual predictions\n",
    "   - Can identify why a student was flagged as at-risk\n",
    "\n",
    "**Business value**: This explainability enables stakeholders to trust the model and take targeted interventions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
