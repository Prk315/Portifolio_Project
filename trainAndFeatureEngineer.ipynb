{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb576864",
   "metadata": {},
   "source": [
    "# Exploration and training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4718892",
   "metadata": {},
   "source": [
    "### This part off the notebook will focus on the exploration off the data and attempts at fitting, training and feature engineering the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95002081",
   "metadata": {},
   "source": [
    "First we add the necessary dependencies and import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791020b4",
   "metadata": {},
   "outputs": [],
   "source": "# Import libraries\nimport pandas as pd\nimport numpy as np\nimport json\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\nfrom sklearn.feature_selection import SelectKBest, f_classif, RFE\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load data from transform.ipynb outputs\nX_train_raw = pd.read_csv('data/X_train_raw.csv')\nX_test_raw = pd.read_csv('data/X_test_raw.csv')\ny_train = pd.read_csv('data/y_train.csv').squeeze()\ny_test = pd.read_csv('data/y_test.csv').squeeze()\n\n# Load column lists\nwith open('data/num_vals.json', 'r') as f:\n    num_vals = json.load(f)\nwith open('data/cat_vals.json', 'r') as f:\n    cat_vals = json.load(f)\n\n# Load preprocessor\npreprocessor = joblib.load('data/preprocessor.joblib')\n\nprint(f\"Training set: {X_train_raw.shape}\")\nprint(f\"Test set: {X_test_raw.shape}\")\nprint(f\"Target classes: {y_train.unique()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e929cf",
   "metadata": {},
   "outputs": [],
   "source": "# Transform data using the fitted preprocessor\nX_train = preprocessor.transform(X_train_raw)\nX_test = preprocessor.transform(X_test_raw)\n\n# Encode target variable\nle = LabelEncoder()\ny_train_encoded = le.fit_transform(y_train)\ny_test_encoded = le.transform(y_test)\n\nprint(f\"Transformed training shape: {X_train.shape}\")\nprint(f\"Transformed test shape: {X_test.shape}\")\nprint(f\"Label mapping: {dict(zip(le.classes_, range(len(le.classes_))))}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "4919c296",
   "metadata": {},
   "outputs": [],
   "source": "## Baseline Models with GridSearchCV\n\nTraining simple classifiers to establish baseline performance before feature engineering."
  },
  {
   "cell_type": "code",
   "id": "xfc6h8ezfh",
   "source": "# Decision Tree Classifier with GridSearchCV\ndt_params = {\n    'max_depth': [3, 5, 10, 15, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'criterion': ['gini', 'entropy']\n}\n\ndt_grid = GridSearchCV(\n    DecisionTreeClassifier(random_state=42),\n    dt_params,\n    cv=5,\n    scoring='f1_weighted',\n    n_jobs=-1,\n    verbose=1\n)\n\ndt_grid.fit(X_train, y_train_encoded)\n\nprint(f\"\\nBest Decision Tree Parameters: {dt_grid.best_params_}\")\nprint(f\"Best CV F1 Score: {dt_grid.best_score_:.4f}\")\n\n# Evaluate on test set\ndt_pred = dt_grid.predict(X_test)\nprint(f\"Test Accuracy: {accuracy_score(y_test_encoded, dt_pred):.4f}\")\nprint(f\"Test F1 Score: {f1_score(y_test_encoded, dt_pred, average='weighted'):.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "e4md56vjw6i",
   "source": "# Logistic Regression with GridSearchCV\nlr_params = {\n    'C': [0.01, 0.1, 1, 10],\n    'penalty': ['l2'],\n    'solver': ['lbfgs', 'newton-cg'],\n    'max_iter': [1000]\n}\n\nlr_grid = GridSearchCV(\n    LogisticRegression(random_state=42),\n    lr_params,\n    cv=5,\n    scoring='f1_weighted',\n    n_jobs=-1,\n    verbose=1\n)\n\nlr_grid.fit(X_train, y_train_encoded)\n\nprint(f\"\\nBest Logistic Regression Parameters: {lr_grid.best_params_}\")\nprint(f\"Best CV F1 Score: {lr_grid.best_score_:.4f}\")\n\n# Evaluate on test set\nlr_pred = lr_grid.predict(X_test)\nprint(f\"Test Accuracy: {accuracy_score(y_test_encoded, lr_pred):.4f}\")\nprint(f\"Test F1 Score: {f1_score(y_test_encoded, lr_pred, average='weighted'):.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "376vyh6gxx2",
   "source": "# K-Nearest Neighbors with GridSearchCV\nknn_params = {\n    'n_neighbors': [3, 5, 7, 9, 11],\n    'weights': ['uniform', 'distance'],\n    'metric': ['euclidean', 'manhattan']\n}\n\nknn_grid = GridSearchCV(\n    KNeighborsClassifier(),\n    knn_params,\n    cv=5,\n    scoring='f1_weighted',\n    n_jobs=-1,\n    verbose=1\n)\n\nknn_grid.fit(X_train, y_train_encoded)\n\nprint(f\"\\nBest KNN Parameters: {knn_grid.best_params_}\")\nprint(f\"Best CV F1 Score: {knn_grid.best_score_:.4f}\")\n\n# Evaluate on test set\nknn_pred = knn_grid.predict(X_test)\nprint(f\"Test Accuracy: {accuracy_score(y_test_encoded, knn_pred):.4f}\")\nprint(f\"Test F1 Score: {f1_score(y_test_encoded, knn_pred, average='weighted'):.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "37ozj03u0zo",
   "source": "# Baseline Results Summary\nbaseline_results = pd.DataFrame({\n    'Model': ['Decision Tree', 'Logistic Regression', 'KNN'],\n    'Best CV F1': [dt_grid.best_score_, lr_grid.best_score_, knn_grid.best_score_],\n    'Test Accuracy': [\n        accuracy_score(y_test_encoded, dt_pred),\n        accuracy_score(y_test_encoded, lr_pred),\n        accuracy_score(y_test_encoded, knn_pred)\n    ],\n    'Test F1': [\n        f1_score(y_test_encoded, dt_pred, average='weighted'),\n        f1_score(y_test_encoded, lr_pred, average='weighted'),\n        f1_score(y_test_encoded, knn_pred, average='weighted')\n    ]\n})\n\nprint(\"=\"*60)\nprint(\"BASELINE RESULTS SUMMARY\")\nprint(\"=\"*60)\nprint(baseline_results.to_string(index=False))\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kgzeu07c2gg",
   "source": "## Feature Engineering\n\nCreating new features based on domain knowledge and analyzing feature importance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "m7pc5m34amc",
   "source": "# Feature Importance from Decision Tree\nfeature_names = preprocessor.get_feature_names_out()\nimportances = dt_grid.best_estimator_.feature_importances_\n\n# Create importance dataframe\nimportance_df = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importances\n}).sort_values('importance', ascending=False)\n\nprint(\"Top 20 Most Important Features (Decision Tree):\")\nprint(importance_df.head(20).to_string(index=False))\n\n# Visualize top features\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 8))\ntop_20 = importance_df.head(20)\nplt.barh(range(len(top_20)), top_20['importance'].values)\nplt.yticks(range(len(top_20)), top_20['feature'].values)\nplt.xlabel('Importance')\nplt.title('Top 20 Feature Importances')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qpb89v1pd5f",
   "source": "# Create engineered features from raw data\ndef engineer_features(df):\n    \"\"\"Create new features based on domain knowledge.\"\"\"\n    df_new = df.copy()\n    \n    # Academic performance aggregates\n    df_new['total_approved'] = (\n        df['Curricular units 1st sem (approved)'] + \n        df['Curricular units 2nd sem (approved)']\n    )\n    \n    df_new['total_enrolled'] = (\n        df['Curricular units 1st sem (enrolled)'] + \n        df['Curricular units 2nd sem (enrolled)']\n    )\n    \n    # Approval rate (handle division by zero)\n    df_new['approval_rate'] = np.where(\n        df_new['total_enrolled'] > 0,\n        df_new['total_approved'] / df_new['total_enrolled'],\n        0\n    )\n    \n    # Grade average\n    df_new['avg_grade'] = (\n        df['Curricular units 1st sem (grade)'] + \n        df['Curricular units 2nd sem (grade)']\n    ) / 2\n    \n    # Semester improvement (2nd sem - 1st sem grades)\n    df_new['grade_improvement'] = (\n        df['Curricular units 2nd sem (grade)'] - \n        df['Curricular units 1st sem (grade)']\n    )\n    \n    # Approval improvement\n    df_new['approval_improvement'] = (\n        df['Curricular units 2nd sem (approved)'] - \n        df['Curricular units 1st sem (approved)']\n    )\n    \n    # Credited units ratio\n    df_new['total_credited'] = (\n        df['Curricular units 1st sem (credited)'] + \n        df['Curricular units 2nd sem (credited)']\n    )\n    \n    # Without evaluations (potential risk indicator)\n    df_new['total_without_eval'] = (\n        df['Curricular units 1st sem (without evaluations)'] + \n        df['Curricular units 2nd sem (without evaluations)']\n    )\n    \n    # Age group (binned)\n    df_new['age_group'] = pd.cut(\n        df['Age at enrollment'], \n        bins=[0, 20, 25, 30, 100], \n        labels=[0, 1, 2, 3]\n    ).astype(int)\n    \n    return df_new\n\n# Apply feature engineering\nX_train_fe = engineer_features(X_train_raw)\nX_test_fe = engineer_features(X_test_raw)\n\nprint(f\"Original features: {X_train_raw.shape[1]}\")\nprint(f\"After engineering: {X_train_fe.shape[1]}\")\nprint(f\"\\nNew features added:\")\nnew_cols = [c for c in X_train_fe.columns if c not in X_train_raw.columns]\nfor col in new_cols:\n    print(f\"  - {col}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "obu7v7c9s6p",
   "source": "# Update column lists with new features\nnum_vals_fe = num_vals + [\n    'total_approved', 'total_enrolled', 'approval_rate', 'avg_grade',\n    'grade_improvement', 'approval_improvement', 'total_credited', \n    'total_without_eval', 'age_group'\n]\n\n# Create new preprocessor with updated columns\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nnum_pipeline_fe = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')), \n    ('scale', StandardScaler())\n])\n\ncat_pipeline_fe = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')), \n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n])\n\npreprocessor_fe = ColumnTransformer([\n    ('num', num_pipeline_fe, num_vals_fe),\n    ('cat', cat_pipeline_fe, cat_vals)\n])\n\n# Transform feature-engineered data\nX_train_fe_transformed = preprocessor_fe.fit_transform(X_train_fe)\nX_test_fe_transformed = preprocessor_fe.transform(X_test_fe)\n\nprint(f\"Feature-engineered training shape: {X_train_fe_transformed.shape}\")\nprint(f\"Feature-engineered test shape: {X_test_fe_transformed.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "j62mr0dvy8d",
   "source": "## Feature Selection\n\nUsing SelectKBest to identify the most predictive features.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xlo2l586nkp",
   "source": "# Feature Selection using SelectKBest\n# Test different k values to find optimal number of features\nk_values = [50, 75, 100, 125, 150]\nk_results = []\n\nfor k in k_values:\n    selector = SelectKBest(f_classif, k=k)\n    X_train_selected = selector.fit_transform(X_train_fe_transformed, y_train_encoded)\n    X_test_selected = selector.transform(X_test_fe_transformed)\n    \n    # Quick evaluation with Decision Tree\n    dt_temp = DecisionTreeClassifier(random_state=42, max_depth=10)\n    cv_scores = cross_val_score(dt_temp, X_train_selected, y_train_encoded, cv=5, scoring='f1_weighted')\n    \n    k_results.append({\n        'k': k,\n        'cv_f1_mean': cv_scores.mean(),\n        'cv_f1_std': cv_scores.std()\n    })\n    print(f\"k={k}: CV F1 = {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n\nk_results_df = pd.DataFrame(k_results)\nbest_k = k_results_df.loc[k_results_df['cv_f1_mean'].idxmax(), 'k']\nprint(f\"\\nBest k value: {int(best_k)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ey20x9mz5nm",
   "source": "# Apply best feature selection\nselector_best = SelectKBest(f_classif, k=int(best_k))\nX_train_final = selector_best.fit_transform(X_train_fe_transformed, y_train_encoded)\nX_test_final = selector_best.transform(X_test_fe_transformed)\n\nprint(f\"Final training shape: {X_train_final.shape}\")\nprint(f\"Final test shape: {X_test_final.shape}\")\n\n# Get selected feature names\nfeature_names_fe = preprocessor_fe.get_feature_names_out()\nselected_mask = selector_best.get_support()\nselected_features = feature_names_fe[selected_mask]\n\nprint(f\"\\nSelected {len(selected_features)} features\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "pti49b5oq3",
   "source": "## Re-train Models with Engineered Features\n\nComparing performance with feature engineering applied.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "96hir8b1j9v",
   "source": "# Decision Tree with Feature Engineering\ndt_grid_fe = GridSearchCV(\n    DecisionTreeClassifier(random_state=42),\n    dt_params,\n    cv=5,\n    scoring='f1_weighted',\n    n_jobs=-1,\n    verbose=1\n)\ndt_grid_fe.fit(X_train_final, y_train_encoded)\n\nprint(f\"\\nBest Decision Tree (FE) Parameters: {dt_grid_fe.best_params_}\")\nprint(f\"Best CV F1 Score: {dt_grid_fe.best_score_:.4f}\")\n\ndt_pred_fe = dt_grid_fe.predict(X_test_final)\nprint(f\"Test Accuracy: {accuracy_score(y_test_encoded, dt_pred_fe):.4f}\")\nprint(f\"Test F1 Score: {f1_score(y_test_encoded, dt_pred_fe, average='weighted'):.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qzym5ux6yx",
   "source": "# Logistic Regression with Feature Engineering\nlr_grid_fe = GridSearchCV(\n    LogisticRegression(random_state=42),\n    lr_params,\n    cv=5,\n    scoring='f1_weighted',\n    n_jobs=-1,\n    verbose=1\n)\nlr_grid_fe.fit(X_train_final, y_train_encoded)\n\nprint(f\"\\nBest Logistic Regression (FE) Parameters: {lr_grid_fe.best_params_}\")\nprint(f\"Best CV F1 Score: {lr_grid_fe.best_score_:.4f}\")\n\nlr_pred_fe = lr_grid_fe.predict(X_test_final)\nprint(f\"Test Accuracy: {accuracy_score(y_test_encoded, lr_pred_fe):.4f}\")\nprint(f\"Test F1 Score: {f1_score(y_test_encoded, lr_pred_fe, average='weighted'):.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3xmdjwoxou5",
   "source": "# KNN with Feature Engineering\nknn_grid_fe = GridSearchCV(\n    KNeighborsClassifier(),\n    knn_params,\n    cv=5,\n    scoring='f1_weighted',\n    n_jobs=-1,\n    verbose=1\n)\nknn_grid_fe.fit(X_train_final, y_train_encoded)\n\nprint(f\"\\nBest KNN (FE) Parameters: {knn_grid_fe.best_params_}\")\nprint(f\"Best CV F1 Score: {knn_grid_fe.best_score_:.4f}\")\n\nknn_pred_fe = knn_grid_fe.predict(X_test_final)\nprint(f\"Test Accuracy: {accuracy_score(y_test_encoded, knn_pred_fe):.4f}\")\nprint(f\"Test F1 Score: {f1_score(y_test_encoded, knn_pred_fe, average='weighted'):.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "tves5jk398k",
   "source": "# Compare Baseline vs Feature Engineered Results\ncomparison_results = pd.DataFrame({\n    'Model': ['Decision Tree', 'Decision Tree (FE)', \n              'Logistic Regression', 'Logistic Regression (FE)',\n              'KNN', 'KNN (FE)'],\n    'CV F1': [\n        dt_grid.best_score_, dt_grid_fe.best_score_,\n        lr_grid.best_score_, lr_grid_fe.best_score_,\n        knn_grid.best_score_, knn_grid_fe.best_score_\n    ],\n    'Test Accuracy': [\n        accuracy_score(y_test_encoded, dt_pred),\n        accuracy_score(y_test_encoded, dt_pred_fe),\n        accuracy_score(y_test_encoded, lr_pred),\n        accuracy_score(y_test_encoded, lr_pred_fe),\n        accuracy_score(y_test_encoded, knn_pred),\n        accuracy_score(y_test_encoded, knn_pred_fe)\n    ],\n    'Test F1': [\n        f1_score(y_test_encoded, dt_pred, average='weighted'),\n        f1_score(y_test_encoded, dt_pred_fe, average='weighted'),\n        f1_score(y_test_encoded, lr_pred, average='weighted'),\n        f1_score(y_test_encoded, lr_pred_fe, average='weighted'),\n        f1_score(y_test_encoded, knn_pred, average='weighted'),\n        f1_score(y_test_encoded, knn_pred_fe, average='weighted')\n    ]\n})\n\nprint(\"=\"*70)\nprint(\"BASELINE vs FEATURE ENGINEERED COMPARISON\")\nprint(\"=\"*70)\nprint(comparison_results.to_string(index=False))\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "78uc4m5azag",
   "source": "## Save Artifacts for Next Notebook\n\nSaving models, data, and preprocessing objects for hyperparameter tuning.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kcajh0vyjv",
   "source": "# Save feature-engineered data\nnp.save('data/X_train_fe.npy', X_train_final)\nnp.save('data/X_test_fe.npy', X_test_final)\nnp.save('data/y_train_encoded.npy', y_train_encoded)\nnp.save('data/y_test_encoded.npy', y_test_encoded)\n\n# Save preprocessor and selector\njoblib.dump(preprocessor_fe, 'data/preprocessor_fe.joblib')\njoblib.dump(selector_best, 'data/selector.joblib')\njoblib.dump(le, 'data/label_encoder.joblib')\n\n# Save feature-engineered raw data for reference\nX_train_fe.to_csv('data/X_train_fe_raw.csv', index=False)\nX_test_fe.to_csv('data/X_test_fe_raw.csv', index=False)\n\n# Save baseline results\nbaseline_results.to_csv('data/baseline_results.csv', index=False)\ncomparison_results.to_csv('data/comparison_results.csv', index=False)\n\n# Save best baseline models\njoblib.dump(dt_grid_fe.best_estimator_, 'data/best_dt.joblib')\njoblib.dump(lr_grid_fe.best_estimator_, 'data/best_lr.joblib')\njoblib.dump(knn_grid_fe.best_estimator_, 'data/best_knn.joblib')\n\n# Save updated column lists\nwith open('data/num_vals_fe.json', 'w') as f:\n    json.dump(num_vals_fe, f)\n\nprint(\"All artifacts saved successfully!\")\nprint(\"\\nSaved files:\")\nprint(\"  - data/X_train_fe.npy, X_test_fe.npy (transformed data)\")\nprint(\"  - data/y_train_encoded.npy, y_test_encoded.npy (encoded labels)\")\nprint(\"  - data/preprocessor_fe.joblib (feature engineering preprocessor)\")\nprint(\"  - data/selector.joblib (feature selector)\")\nprint(\"  - data/label_encoder.joblib (target encoder)\")\nprint(\"  - data/best_dt.joblib, best_lr.joblib, best_knn.joblib (baseline models)\")\nprint(\"  - data/baseline_results.csv, comparison_results.csv (results)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}